This series of tests is designed to see how the method performs on a small-scale example. Rounds 1 and 2 only had info from mol1, so I changed the chemical reactions before round3.

Round 4 was different in that the measurement error was cranked down to sd=1. The first run showed promise. I repeated it in rounds 5 and 6, and it was a disaster! The bandwidth collapsed, and the posterior converged on a small spot far from the true rates.

I need something simpler. I’m going to scrap this for a one-molecule system. Rounds 7-9 cuts out all the frills: one molecule, production, decay. 1000 or 10,000 particles, log-space centered priors. Error sd of 1. Misplaced the ground truth on 7/8, so disregard them. Round 9, 10,000 particles, looks promising, especially early on, but it degenerates. I tried round 10 with 100,000 particles and round 11 with 10,000 particles but with sd back up to 10. Round 11 looks promising but ultimately collapses on the last round and picks the wrong mode. 

Rounds 12-14 increased the bandwidth multiplier from 0.1 (which Wilkinson suggested) to 1, and then 10, and then 100 (each round used 10,000 particles, noise sd = 10). Same trend: promising, but collapses too much. These did look a little better—collapse was slower and less disastrous. Round 14 ran slowly because wide bandwidth means some simulations have fast rxn rates. I canceled it and bumped bw mult down to 50. 

By a few stages in, the bandwidth reached NaN . . . oops. Acceptance rates were higher than I’ve ever seen: they never went below 50%! In particular, there used to be about 2% acceptance in the first round. Alas, the plots failed because the samples contained NaN’s, and I’d rather keep going than excavate. Amazing that the sampler even finished: what the fuck does Gillespie do with NaN’s? I toned the multiplier down to 25 for round 15. NaN’s again.

For round 16, I tried to deal with the the bandwidth blowup by modifying so that anything above 1 gets set back to 1. (The multiplier may exceed 1; the bandwidth may not.) Then I ran a test with a bw multiplier of 25. The bandwidth collapsed on one axis, but not the other: the one where it didn’t, I got a nice marginal plot. To keep that from happening, I  decided to tie the two together: compute both but use the mean. Using that trick, round 17 was suspicious, but ok.


Some general notes from today:

The bandwidths here keep collapsing to values well below 10^-4. This is for parameters already in log-space. The Silverman bandwidth is linear in the sample sd, so there could be a cascading effect: big bandwidths make for a wider cloud and a bigger estimated bandwidth next time around, and vice versa.

The sequence of distributions sometimes takes on a spatial trend, but more often it sits still or degenerates. On round 10, you can see, between rounds four and five, three co-occuring things. First, an apparent degeneration of the particle cloud. Second, a high rejection rate. Third, a little hop in the path of the simulated data. I’d suggest these are all related: as soon as the (stochastic!) system takes an even moderately unexpected turn, the sampler says “no no no no no” to all the proposals. 

I have got the profiler turned on, and it looks reasonable. Cost sits mostly inside Gillespie and pMCMC_single_stage!.
