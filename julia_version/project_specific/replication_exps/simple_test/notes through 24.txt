This series of tests is designed to see how the method performs on a small-scale example. Rounds 1 and 2 only had info from mol1, so I changed the chemical reactions before round3.

Round 4 was different in that the measurement error was cranked down to sd=1. The first run showed promise. I repeated it in rounds 5 and 6, and it was a disaster! The bandwidth collapsed, and the posterior converged on a small spot far from the true rates.

I need something simpler. I’m going to scrap this for a one-molecule system. Rounds 7-9 cuts out all the frills: one molecule, production, decay. 1000 or 10,000 particles, log-space centered priors. Error sd of 1. Misplaced the ground truth on 7/8, so disregard them. Round 9, 10,000 particles, looks promising, especially early on, but it degenerates. I tried round 10 with 100,000 particles and round 11 with 10,000 particles but with sd back up to 10. Round 11 looks promising but ultimately collapses on the last round and picks the wrong mode. 

Rounds 12-14 increased the bandwidth multiplier from 0.1 (which Wilkinson suggested) to 1, and then 10, and then 100 (each round used 10,000 particles, noise sd = 10). Same trend: promising, but collapses too much. These did look a little better—collapse was slower and less disastrous. Round 14 ran slowly because wide bandwidth means some simulations have fast rxn rates. I canceled it and bumped bw mult down to 50. 

By a few stages in, the bandwidth reached NaN . . . oops. Acceptance rates were higher than I’ve ever seen: they never went below 50%! In particular, there used to be about 2% acceptance in the first round. Alas, the plots failed because the samples contained NaN’s, and I’d rather keep going than excavate. Amazing that the sampler even finished: what the fuck does Gillespie do with NaN’s? I toned the multiplier down to 25 for round 15. NaN’s again.

For round 16, I tried to deal with the the bandwidth blowup by modifying so that anything above 1 gets set back to 1. (The multiplier may exceed 1; the bandwidth may not.) Then I ran a test with a bw multiplier of 25. The bandwidth collapsed on one axis, but not the other: the one where it didn’t, I got a nice marginal plot. To keep that from happening, I decided to tie the two together: compute both but use the mean. Using that trick, round 17 was suspicious, but ok. Round 18 confirmed my suspicions: the particle cloud fled to the far reaches of the plot. Fucker.

Overnight, I ran Round 19 with 1_000_000 particles, changing the time-scale to  
t_interval = 30.0
num_intervals = 48.0

The posterior was wide, centered in the correct place, but didn’t seem to change much at each datum. For comparison, rounds 20-21 used the same time-scale, but 10,000 particles. Unfortunately, they both worked, too. So, maybe it wasn’t the million particles that helped. Maybe it was the time-scale. 

Round 22 used 100,000 particles with the original timescale, bw multiplier of 25, bw averaging trick, and bw_max of 1. BW’s started around 0.05 and gradually climbed to 0.50. 0.2. Like the analogous 10,000 particle trial, round 17, it converged to a blob above the true params. Also showed similar patterns in intermediate stages: a square with a stretched-out top-right corner, with the bottom or the LHS occasionally vanishing. Round 23 was a repeat of these settings. The bw stayed smaller this time, below 0.15. The acceptance rates were high again. But, the posterior collapsed into something concentrated above the true value.


Round 24 modified the initial condition to be 500 molecules. Hopefully, this will result in data containing more info about the parameters. The actual result: posterior collapses to a blob below the true params.

Some general notes from today:

The bandwidths here keep collapsing to values well below 10^-4. This is for parameters already in log-space. The Silverman bandwidth is linear in the sample sd, so there could be a cascading effect: big bandwidths make for a wider cloud and a bigger estimated bandwidth next time around, and vice versa.

The sequence of distributions sometimes takes on a spatial trend, but more often it sits still or degenerates. On round 10, you can see, between rounds four and five, three co-occuring things. First, an apparent degeneration of the particle cloud. Second, a high rejection rate. Third, a little hop in the path of the simulated data. I’d suggest these are all related: as soon as the (stochastic!) system takes an even moderately unexpected turn, the sampler says “no no no no no” to all the proposals. 

I have got the profiler turned on, and it looks reasonable. Cost sits mostly inside Gillespie and pMCMC_single_stage!. I should probably try to use that sparsity in rxn-matrix etc.
